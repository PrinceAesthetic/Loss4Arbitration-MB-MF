{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "from scipy.special import softmax \n",
    "import time \n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns \n",
    "\n",
    "import sys \n",
    "sys.path.append(\"..\") \n",
    "from utils.viz import viz \n",
    "viz.get_style()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "class two_stage:\n",
    "    nS = 9\n",
    "    nA = 2\n",
    "    nC = 2 \n",
    "    nR = 3\n",
    "    s_termination = list(range(5,9))\n",
    "    \n",
    "    def __init__(self,seed=1234):\n",
    "        '''A MDP is a 5-element-tuple\n",
    "\n",
    "        S: state space\n",
    "        A: action space\n",
    "        T: transition function\n",
    "        C: certainty\n",
    "        R: reward condition\n",
    "        '''\n",
    "        self.rng = np.random.RandomState(seed)\n",
    "        self.nS = two_stage.nS\n",
    "        self.nA = two_stage.nA\n",
    "        self.nC = two_stage.nC \n",
    "        self._init_state()\n",
    "        self._init_action()\n",
    "        self._init_trans_fn()\n",
    "        self._init_reward()\n",
    "\n",
    "\n",
    "    \n",
    "    def _init_state(self):\n",
    "        self.S  = np.arange(two_stage.nS)\n",
    "\n",
    "    def _init_action(self):\n",
    "        self.A  = np.arange(two_stage.nA)\n",
    "\n",
    "    def _init_certainty(self):\n",
    "        self.C  = np.arange(two_stage.nC)\n",
    "\n",
    "    def _init_reward(self):\n",
    "        self.R  = np.arange(two_stage.nR)\n",
    "\n",
    "    def _init_trans_fn(self):\n",
    "        '''T(s'|s,a)'''\n",
    "        def pro(C):\n",
    "            if C == 0:\n",
    "                prob = 0.9\n",
    "            elif C == 1:\n",
    "                prob = 0.5\n",
    "            return prob\n",
    "        \n",
    "        self.t = {\n",
    "                0: {0: {0: [0, pro(0), 1-pro(0), 0, 0, 0, 0, 0, 0],\n",
    "                        1: [0, pro(1), 1-pro(1), 0, 0, 0, 0, 0, 0]},\n",
    "                    1: {0: [0, 0, 0, pro(0), 1-pro(0), 0, 0, 0, 0],\n",
    "                        1: [0, 0, 0, pro(1), 1-pro(1), 0, 0, 0, 0]},},\n",
    "\n",
    "                1: {0: {0: [0, 0, 0, 0, 0, 0, 1-pro(0), pro(0), 0],\n",
    "                        1: [0, 0, 0, 0, 0, 0, 1-pro(1), pro(1), 0]},\n",
    "                    1: {0: [0, 0, 0, 0, 0, 1-pro(0), pro(0), 0, 0],\n",
    "                        1: [0, 0, 0, 0, 0, 1-pro(1), pro(1), 0, 0]},},\n",
    "\n",
    "                2: {0: {0: [0, 0, 0, 0, 0, 0, 1-pro(0), pro(0), 0],\n",
    "                        1: [0, 0, 0, 0, 0, 0, 1-pro(1), pro(1), 0]},\n",
    "                    1: {0: [0, 0, 0, 0, 0, 1-pro(0), 0, pro(0), 0],\n",
    "                        1: [0, 0, 0, 0, 0, 1-pro(1), 0, pro(1), 0]},},\n",
    "                \n",
    "                3: {0: {0: [0, 0, 0, 0, 0, 0, 0, pro(0), 1-pro(0)],\n",
    "                        1: [0, 0, 0, 0, 0, 0, 0, pro(1), 1-pro(1)]},\n",
    "                    1: {0: [0, 0, 0, 0, 0, 1-pro(0), 0, 0, pro(0)],\n",
    "                        1: [0, 0, 0, 0, 0, 1-pro(1), 0, 0, pro(1)]},},\n",
    "\n",
    "                4: {0: {0: [0, 0, 0, 0, 0, 1-pro(0), 0, pro(0), 0],\n",
    "                        1: [0, 0, 0, 0, 0, 1-pro(1), 0, pro(1), 0]},\n",
    "                    1: {0: [0, 0, 0, 0, 0, pro(0), 0, 0, 1-pro(0)],\n",
    "                        1: [0, 0, 0, 0, 0, pro(1), 0, 0, 1-pro(1)]},},\n",
    "                }\n",
    "\n",
    "        \n",
    "    def trans_fn(self,s,a,C):\n",
    "        self.T = self.t[s][a][C]\n",
    "        return self.T         \n",
    "    \n",
    "    def _init_reward(self):\n",
    "        '''R(r|s',a)''' \n",
    "    \n",
    "        self.r = {\n",
    "                    0:[0,0,0,0,0,0,.1,.2,.4],\n",
    "                    1:[0,0,0,0,0,0,.1,0,0],\n",
    "                    2:[0,0,0,0,0,0,0,.2,0],\n",
    "                }   \n",
    "          \n",
    "    def reward_fn(self,W,s):\n",
    "        self.R = self.r[W][s]\n",
    "        return self.R\n",
    "    \n",
    "    def reset(self):\n",
    "        '''always start with state=0\n",
    "        '''\n",
    "        self.s = 0\n",
    "        self.done = False \n",
    "        return self.s \n",
    "    \n",
    "    def step(self,a,C,W):\n",
    "        \n",
    "        # get next state \n",
    "        p_s_next = self.trans_fn(self.s,a,C)\n",
    "        s_next = self.rng.choice(self.S, p=p_s_next)\n",
    "        # get the reward\n",
    "        obj_r = self.reward_fn(W,s_next)\n",
    "        # check the termination\n",
    "        if s_next > 4: self.done = True \n",
    "        # move on \n",
    "        self.s = s_next \n",
    "\n",
    "        return self.s, obj_r, self.done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sim(agent_fn, params, n_episode = 5000, seed=873): \n",
    "    # agent_fn = lossarbi specific,high; flexible,low; specific, high; flexible, low\n",
    "    cols = ['a1','s2','r1','a2','s3','r2']\n",
    "    sim_data = {col: [] for col in cols}\n",
    "    nS, nA= 9, 2 \n",
    "    rng = np.random.RandomState(seed)\n",
    "    agent = agent_fn(nS, nA, rng, params=params)\n",
    "    env = two_stage()\n",
    "    for epi in range(n_episode):\n",
    "        # stage 1\n",
    "        s1 = env.reset()          # get state \n",
    "        a1 = agent.make_move(s1)           # get action\n",
    "        # stage 2\n",
    "        s2, r1, done = env.step(a1,C=0,W=0)  # get state; C is the uncertainty, W is the reward condition\n",
    "        agent.learn1(s1, a1, s2, r1)\n",
    "       \n",
    "        a2 = agent.make_move(s2)           # get action \n",
    "        s3, r2, done = env.step(a2,C=0,W=0)      # get reward\n",
    "        agent.learn2(s1, a1, s2, r1, a2, s3, r2)   \n",
    "        # save\n",
    "        \n",
    "        sim_data['a1'].append(a1)\n",
    "        sim_data['s2'].append(s2)\n",
    "        sim_data['r1'].append(r1)\n",
    "        sim_data['a2'].append(a2)\n",
    "        sim_data['s3'].append(s3)\n",
    "        sim_data['r2'].append(r2)\n",
    "    \n",
    "    return sim_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The behavior of arbitrator who minimizes loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class lossarbi:\n",
    "    '''SARSA + Model-based\n",
    "    '''\n",
    "\n",
    "    def __init__(self, nS, nA, rng, params):\n",
    "        self.nS = nS\n",
    "        self.nA = nA\n",
    "        self.rng = rng\n",
    "        self.Q_mf = np.zeros([nS, nA]) \n",
    "        self.Q_mb = np.zeros([nS, nA])\n",
    "        '''\n",
    "        Agent's perception of the environment, \n",
    "        in which he starts with equal probabilities \n",
    "        and gradually learn the approximate transition matrix\n",
    "        '''  \n",
    "        self.P    = np.zeros([nS, nA, nS])\n",
    "        self.P[0,0,[1,2]] = .5\n",
    "        self.P[0,1,[3,4]] = .5\n",
    "        self.P[1,0,[6,7]] = .5\n",
    "        self.P[1,1,[5,6]] = .5\n",
    "        self.P[2,0,[5,6]] = .5\n",
    "        self.P[2,1,[5,7]] = .5\n",
    "        self.P[3,0,[7,8]] = .5\n",
    "        self.P[3,1,[5,8]] = .5\n",
    "        self.P[4,0,[5,7]] = .5\n",
    "        self.P[4,1,[5,8]] = .5\n",
    "        self.P[5:,:,0] = 1\n",
    "        self.alpha1 = params['alpha1']\n",
    "        self.alpha2 = params['alpha2']\n",
    "        self.beta1  = params['beta1']\n",
    "        self.beta2  = params['beta2']\n",
    "        self.lmbda  = params['lmbda']\n",
    "        self.alpha_u = params['alpha_u']\n",
    "        self.theta1 = params['theta1'] # transition matrix learning rate\n",
    "        self.p      = params['p']\n",
    "        self.eta    = params['eta']\n",
    "        self.u      = 0\n",
    "        self.w      = 0.5\n",
    "        self.n = 0 # for model based learner, kind of reward condition\n",
    "        self.reward = np.zeros([3, nS])\n",
    "        self.reward[0:,5:] = [[0, .1, .2 ,.4],[0,.1,0,0],[0,0,.2,0]] # should the model based learner learn the reward through trials? \n",
    "        #or should we just suppose he knows it immediately\n",
    "        self.rep_a  = np.zeros([self.nA])\n",
    "  \n",
    "\n",
    "    def make_move(self, s):\n",
    "        q_mf = self.Q_mf[s, :]\n",
    "        q_mb = self.Q_mb[s, :]\n",
    "        q_net = self.w*q_mb + (1-self.w)*q_mf\n",
    "        beta = self.beta1 if s==0 else self.beta2\n",
    "        q = q_net + self.p*self.rep_a\n",
    "        pi = softmax(beta*q)\n",
    "        return self.rng.choice(self.nA, p=pi) \n",
    "    \n",
    "    def learn1(self, s1, a1, s2, r1): # r1 = 0, so no use\n",
    "        #model free update\n",
    "        delta = r1 + np.max(self.Q_mf[s2,:]) - self.Q_mf[s1, a1]\n",
    "        self.Q_mf[s1, a1] += self.alpha2 * delta\n",
    "\n",
    "        #model based update\n",
    "        self.P[s1, a1, s2] = self.P[s1, a1, s2] * (1 - self.theta1) + self.theta1\n",
    "        if a1 == 0:\n",
    "            self.P[s1, a1, 3-s2] = 1 - self.P[s1, a1, s2]\n",
    "            self.Q_mb[s1, a1] = self.P[s1, a1, s2] * np.max(self.Q_mb[s2, :]) + self.P[s1, a1, 3-s2] * np.max(self.Q_mb[3-s2, :])\n",
    "        else:\n",
    "            self.P[s1, a1, 7-s2] = 1 - self.P[s1, a1, s2]\n",
    "            self.Q_mb[s1, a1] = self.P[s1, a1, s2] * np.max(self.Q_mb[s2, :]) + self.P[s1, a1, 7-s2] * np.max(self.Q_mb[7-s2, :])\n",
    "        \n",
    "        # update the arbitration weight w\n",
    "        w1 = self.w\n",
    "        grad_u = (self.w*self.Q_mb[s1,a1] + (1-self.w)*self.Q_mf[s1,a1] - (w1 * np.max(self.Q_mb[s2,:]) + (1-w1) * np.max(self.Q_mf[s2,:]))) * \\\n",
    "            (self.Q_mb[s1,a1] - self.Q_mf[s1,a1]) * (1/(1+np.exp(-self.u))**2) * np.exp(-self.u)\n",
    "        self.u = self.u - self.alpha_u*grad_u\n",
    "        self.w = 1/(1+np.exp(-self.u))\n",
    "       \n",
    "\n",
    "    def learn2(self, s1, a1, s2, r1, a2, s3, r2):\n",
    "        # model-free update \n",
    "        q_hat2 = self.Q_mf[s2, a2].copy()\n",
    "        q_hat1 = self.Q_mf[s1, a1].copy()\n",
    "        delta2 = r2 - q_hat2\n",
    "        delta1 = q_hat2 - q_hat1\n",
    "        self.Q_mf[s2, a2] += self.alpha2*delta2\n",
    "        self.Q_mf[s1, a1] += self.alpha1*(delta1 + self.lmbda*delta2) #self.lmbda*delta2 takes into account the update of Q(s2,a2),is that necessary?\n",
    "\n",
    "        # model-based update\n",
    "        ##level 2 update\n",
    "        ###first update the perception of the environment\n",
    "        self.P[s2, a2, s3] = self.P[s2, a2, s3] * (1 - self.theta1) + self.theta1\n",
    "        for i in range(9):\n",
    "            if self.P[s2, a2, i] != 0 and i != s3:\n",
    "                self.P[s2, a2, i] = 1 - self.P[s2, a2, s3].copy() ###then update the Q value for model based learner\n",
    "                self.Q_mb[s2, a2] = self.P[s2, a2, s3] * self.reward[self.n, s3] + self.P[s2, a2, i] * self.reward[self.n, i]\n",
    "\n",
    "        ##level 1 update\n",
    "        if a1 == 0:\n",
    "            self.Q_mb[s1, a1] = self.P[s1, a1, s2] * np.max(self.Q_mb[s2, :]) + self.P[s1, a1, 3-s2] * np.max(self.Q_mb[3-s2, :])\n",
    "        else:\n",
    "            self.Q_mb[s1, a1] = self.P[s1, a1, s2] * np.max(self.Q_mb[s2, :]) + self.P[s1, a1, 7-s2] * np.max(self.Q_mb[7-s2, :])\n",
    "        \n",
    "        \n",
    "        # update perseveration\n",
    "        self.rep_a = np.eye(self.nA)[a1]\n",
    "       \n",
    "        # update w\n",
    "        # Loss = (wq_mb(s1,a1) + (1-w)q_mf(s1,a1) - (0 + eta * r2 ))^2\n",
    "        grad_u = (self.w*self.Q_mb[s1,a1] + (1-self.w)*self.Q_mf[s1,a1] - self.eta*r2) * (self.Q_mb[s1,a1] - self.Q_mf[s1,a1]) * (1/(1+np.exp(-self.u))**2) * np.exp(-self.u)\n",
    "        self.u = self.u - self.alpha_u*grad_u\n",
    "        self.w = 1/(1+np.exp(-self.u))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_fn = lossarbi\n",
    "params = {'alpha1': .4,\n",
    "          'alpha2': .5, \n",
    "          'beta1': 6, \n",
    "          'beta2': 4, \n",
    "          'lmbda': .6,\n",
    "          'alpha_u': .5,\n",
    "          'p': .1,\n",
    "          'eta':.99,\n",
    "          'theta1':0.3\n",
    "          }\n",
    "sim_LA_data = sim(agent_fn, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sim_LA_df = pd.DataFrame(sim_LA_data)\n",
    "\n",
    "sim_LA_df.to_excel(\"experiment_data.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
